---
title: 'STOR 455 Homework #5'
subtitle: 40 points - Due Wednesday 3/20 at 5:00pm
geometry: margin = 2.25cm
author: 'Group 10'
output:
  pdf_document: default
---

__Directions:__ For parts 7 and 10 you should work together, but these parts must be __submitted individually__ by each group member. For parts 8 and 9, you must have only __one submission per group__. There will be separate places on Gradescope to submit the individual vs group work. 

__Situation:__ Can we predict the selling price of a house in Ames, Iowa based on recorded features of the house? That is your task for this assignment. Each team will get a dataset with information on forty potential predictors and the selling price (in $1,000’s) for a sample of homes. The data sets for your group are AmesTrain??.csv and AmesTest??.csv (where ?? corresponds to your group number) A separate file identifies the variables in the Ames Housing data and explains some of the coding.

#### Part 7. Cross-validation: ####
In some situations, a model might fit the peculiarities of a specific sample of data well, but not reflect structure that is really present in the population. A good test for how your model might work on "real" house prices can be simulated by seeing how well your fitted model does at predicting prices that were NOT in your original sample. This is why we reserved an additional 200 cases as a holdout sample in AmesTest??.csv. Use the group number and AmesTest??.csv corresponding to your group number for homework #3. Import your holdout test data and 

#### Part 8. Find a “fancy model”:

Again using AmesTrain??.csv, where ?? corresponds to your new group
number in homework #5, to build a regression model to predict Price. In
addition to the quantitative predictors, you may now consider models
with

#We load in the data and factor our categorical variables. We also
adjust the #year built/remodel to be years since built/remodel and we
remove 'order'

```{r}
AmesTrainingData <- read.csv('AmesTrain10.csv', stringsAsFactors = TRUE)
AmesTrainingData <- na.omit(AmesTrainingData)
AmesTrainingData <- AmesTrainingData[-1] 
age_built <- 2010 - AmesTrainingData$YearBuilt
age_remodel <- 2010 - AmesTrainingData$YearRemodel
AmesTrainingData <- subset(AmesTrainingData, select = -c(YearBuilt, YearRemodel))

AmesTrainingData$Quality <- factor(AmesTrainingData$Quality)
AmesTrainingData$Condition <- factor(AmesTrainingData$Condition)
```

#we build a full model as a baseline with all of the predictors included. 
#and we plot each of the predictors against price to see if there are any with clear shape

```{r, fig.show='hide'}
full_model <- lm(Price ~ ., data=AmesTrainingData)
summary(full_model)
plot(Price ~ ., data=AmesTrainingData)
```


```{r}
AmesTrainingData_adjusted <- AmesTrainingData %>%
  mutate(Price = log(Price),
         LotArea = log(LotArea))

first_model <- lm(Price ~ . + I(age_built^2) + I(GarageCars^2)+ I(GarageSF^2)+ TotalRooms * Bedroom + LotArea * LotFrontage + GarageSF * GarageCars, data = AmesTrainingData_adjusted)

summary(first_model)
```

#Firstly, we opted to transform the price variable by taking its logarithm. This decision was based on its ability to mitigate the influence of extreme values, thus enhancing the fit of our model, particularly with respect to its curvature.
# we can check the effectiveness of such transformation by comparing the model with and without:

```{r, echo=TRUE,  results='hide'}
summary(lm(Price~., AmesTrainingData))
summary(lm(log(Price)~., AmesTrainingData))
```

#Similarly, we applied a logarithmic transformation to the LotArea variable. Our rationale behind this choice stems from observing a curving trend within the data, which logarithmic adjustment tends to alleviate effectively. 

#Similarly we found that applying log() to LotArea improves the amount of variability in price that is explained #This process was repeated for other transformations and additions to make sure they will be beneficial

#For variables such as age built and garage cars, which exhibited polynomial trends according to our visual inspection of scatter plots, we decided to employ polynomial transformations. This method allowed us to capture the non-linear relationships more accurately, thereby improving the model's performance.

#Additionally, recognizing the high correlation between total rooms and total bedrooms, we deemed it essential to include an interaction term between these two variables. By doing so, we aimed to capture any nuanced interplay between these features, thus enriching the model's predictive capacity.

#Similarly, we identified a significant correlation between lot area and lot frontage, prompting us to incorporate an interaction term for these variables as well. This approach enables us to account for their mutual influence more effectively, enhancing the model's robustness.

#Finally, given the strong correlation between garage cars and garage square footage, we decided to handle them in a similar manner, introducing an interaction term to capture their combined effect accurately.

```{r}
MSE2 <- (summary(first_model)$sigma)^2

AmesModNone2 <- lm(Price ~ 1, AmesTrainingData_adjusted)

#step(first_model, scale = MSE2)
#step(AmesModNone2, scope = list(upper=first_model), scale=MSE2, direction='forward')
simple_model <- step(AmesModNone2, scale = MSE2, scope = list(upper = first_model), trace = FALSE)

summary(simple_model)

```

#### Part 9: Cross-validation for your “fancy” model

```{r}
ames10_holdout <- read.csv('AmesTest10.csv')
ames10_holdout$Quality <- factor(ames10_holdout$Quality)
ames10_holdout$Condition <- factor(ames10_holdout$Condition)
```

We will compute the predicted Price for each of the cases in the holdout
test sample, using our new model.

```{r}
predictions <- predict(simple_model, newdata = ames10_holdout)
head(predictions)
```

As in part 7 we will compute the residuals for the 200 holdout cases.

```{r}
holdout_resid <- log(ames10_holdout$Price) - predictions
```

And find the mean and standard deviation of these residuals.

```{r}
mean(holdout_resid)
sd(holdout_resid)
```

The mean value of our residuals is incredibly close to zero which is
ideal. The standard deviation of .0766 seems very low but this is in
terms of log(Price) which is much smaller than price.

We will construct a plot to determine if they are normally distributed.

```{r}
qqnorm(holdout_resid)
qqline(holdout_resid)
```

The residuals appear much more normally distributed than in previous
models. There are potentially one or two extreme points.

Compute the correlation between the predicted values and actual prices
for the holdout sample. This is known as the cross-validation
correlation.

```{r}
cor(predictions, ames10_holdout$Price)
```

Square the cross-validation correlation to get an $R^{2}$ value.

```{r}
cor(predictions, ames10_holdout$Price)^2
```

Now subtract it from the original multiple $R^{2}$ of the training
sample to find the shrinkage.

```{r}
summary(simple_model2)$r.squared - cor(predictions, ames10_holdout$Price)^2
```

The shrinkage is very small so it appears we are not over fitting our
training data.

#### Part 10. Final Model ####  

Again, you may choose to make some additional adjustments to your model after considering the final residual analysis. If you do so, please explain what (and why) you did and provide the summary() for your new final model.
    
Suppose that you are interested in a house in Ames that has the characteristics listed below. Construct a 95% confidence interval for the mean price of such houses.

A 2 story 11 room home, built in 1987 and remodeled in 1999 on a 21540 sq. ft. lot with 328 feet of road frontage. Overall quality is good (7) and condition is average (5). The quality and condition of the exterior are both good (Gd) and it has a poured concrete foundation. There is an 757 sq. foot basement that has excellent height, but is completely unfinished and has no bath facilities. Heating comes from a gas air furnace that is in excellent condition and there is central air conditioning. The house has 2432 sq. ft. of living space above ground, 1485 on the first floor and 947 on the second, with 4 bedrooms, 2 full and one half baths, and 1 fireplace. The 2 car, built-in garage has 588 sq. ft. of space and is average (TA) for both quality and construction. The only porches or decks is a 205 sq. ft. open porch in the front. 
